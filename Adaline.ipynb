{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No notebook anterior, nós aprendemos sobre o Perceptron. Vimos como ele aprende e como pode ser utilizado tanto para classificação binária quanto para regressão linear. Nesse notebook, nós veremos um algoritmo muito parecido com o Perceptron, mais conhecido como __Adaline__, que foi uma proposta de melhoria ao algoritmo original do Perceptron. Veremos as semelhanças e diferenças entre os dois algoritmos e iremos implementá-lo utilizando python e numpy. Por fim, vamos aplicar nos mesmos problemas de classificação do notebook do Perceptron para entender de fato suas diferenças. __O código para utilizar o Adaline em problemas de regressão é exatamente o mesmo do perceptron__.\n",
    "\n",
    "__Objetivos__:\n",
    "\n",
    "- Entender as diferenças entre os algoritmos do Perceptron e Adaline.\n",
    "- Implementar o Adaline e seu modelo de aprendizado em Python puro e Numpy\n",
    "- Utilizar o Adaline para classificação e regressão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sumário"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Introdução](#Introdução)\n",
    "\n",
    "[Regra de Aprendizado do Adaline](#Regra-de-Aprendizado-do-Adaline)\n",
    "\n",
    "[Classificação](#Classificação)\n",
    "- [Porta AND/OR](#Porta-AND/OR)\n",
    "- [Exercício de Classificação](#Exerc%C3%ADcio-de-Classificação)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-20T12:53:30.345746Z",
     "start_time": "2017-09-20T12:52:48.057739Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vanessasoares/anaconda3/envs/mpdl/lib/python3.8/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.samples_generator module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from random import random\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poucos meses após a publicação do teorema da convergência do Perceptron por Rosenblatt, os engenheiros da Universidade de Stanford, Bernard Widrow e Marcian Hoff, publicaram um trabalho descrevendo uma rede neural muito parecida com o Perceptron, a __Adaline__ (do inglês _ADAptive LINear Element_). Porém, ao invés de utilizar a função _step_ como função de ativação, a __Adaline utiliza função de ativação linear e tem uma nova regra de aprendizado supervisionado__, conhecida como __regra de Widrow-Hoff__ (ou __regra delta__, ou ainda __regra LMS__). \n",
    "\n",
    "De fato, tanto o Perceptron quanto o Adaline possuem muitas características semelhantes e __é comum ver o pessoal confundindo o Perceptron com o Adaline__. Entre as principais semelhanças, podemos destacar:\n",
    "- Ambos possuem __apenas um neurônio de N entradas e apenas uma saída. Não há camadas escondidas__.\n",
    "- Ambos são __classificadores lineares binários__ por definição, mas podemos adaptá-los para efetuar __regressão linear__, da mesma forma como vimos no notebook sobre o Perceptron. __Na verdade, o código para treinar um Adaline para regressão é o mesmo de um Perceptron__.\n",
    "- Ambos tem o **método de aprendizagem _online_**. Isto é, a atualização dos pesos é efetuada amostra por amostra.\n",
    "- Ambos tem uma **função _step_ para classificação**. Porém, ao contrário do Perceptron, __na Adaline ela não é utilizada na atualização dos pesos__. Nós veremos por que a seguir.\n",
    "\n",
    "Porém, a principal diferença entre o Perceptron e a Adaline é que o Perceptron utiliza os labels das classes para fazer a atualização dos pesos, enquanto __a Adaline utiliza o resultado da função de ativação (linear) como valor contínuo de predição__. Isto é, ao invés da saída ser discreta como no Perceptron (0 ou 1), __na Adaline a saída pode ser qualquer valor contínuo__. Essa diferença fica mais clara quando vemos a figura a seguir:\n",
    "\n",
    "<img src=\"images/comparacao_perceptron_adaline.png\">\n",
    "[Fonte](https://www.quora.com/What-is-the-difference-between-a-Perceptron-Adaline-and-neural-network-model)\n",
    "\n",
    "Repare, como dito, que ambos têm a função _step_. No Perceptron, ela é utilizada como função de ativação. No Adaline, por sua vez, a função de ativação é linear e a funcão _step_ é utilizada para gerar a predição. \n",
    "\n",
    "Por calcular a saída como um valor contínuo, __muitos consideram o Adaline mais poderoso__, uma vez que a diferença entre a saída desejada e o valor predito ($y_i - \\widehat{y}_i$) nos diz agora \"o quanto estamos certos ou errados\". __Na prática, isso faz com o que o Adaline tente encontrar a \"melhor solução\" para o problema, ao invés de somente uma \"solução adequada\"__. Tomando como exemplo a figura abaixo, o Perceptron pode encontrar diversas retas que separam as classes, enquanto o Adaline tenta encontrar a melhor reta que separa as classes.\n",
    "\n",
    "<img src=\"images/hiperplanos_perceptron_adaline.png\" width='700'>\n",
    "\n",
    "[Fonte](http://www.barbon.com.br/wp-content/uploads/2013/08/RNA_Aula4.pdf)\n",
    "\n",
    "Ok, mas como isso muda o aprendizado? É o que veremos a seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Regra de Aprendizado do Adaline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A atualização dos pesos do Adaline é dada pela mesma fórmula do Perceptron:\n",
    "\n",
    "$$w_i = w_i + \\lambda(y_i - \\widehat{y}_i)x_i$$\n",
    "\n",
    "Onde $\\lambda$ é a __taxa de aprendizagem__.\n",
    "\n",
    "Mas você já imaginou da onde vem essa fórmula? Em primeiro lugar, o método de atualização dos pesos é baseado na __Regra Delta__ (*Delta Rule*). Sendo $\\overrightarrow{w} = \\{w_1, w_2, ..., w_D\\}$, a atualização dos pesos é dada por:\n",
    "\n",
    "$$\\overrightarrow{w} = \\overrightarrow{w} - \\Delta{\\overrightarrow{w}}$$\n",
    "\n",
    "em que:\n",
    "\n",
    "$$\\Delta{\\overrightarrow{w}} = \\lambda\\nabla E(\\overrightarrow{w})$$\n",
    "\n",
    "Sendo $\\nabla E(\\overrightarrow{w})$ o gradiente de uma função que depende de $\\overrightarrow{w}$ e que queremos minimizar.\n",
    "\n",
    "No caso do Adaline, __a função de custo é dada pela soma dos erros quadrados__:\n",
    "\n",
    "$$J(w) = \\frac{1}{2}\\sum_{i}^N (y_i - \\widehat{y}_i)^2$$\n",
    "\n",
    "Onde $N$ é a quantidade de amostras nos dados, e as demais variáveis representam as mesmas vistas anteriormente. Repare que a função de custo é quase uma _Mean Squared Error (MSE)_, só que ao invés de dividir por $N$, estamos dividindo por 2 o resultado do somatório. O por quê disso será entendido mais a frente na demonstração.\n",
    "\n",
    "Queremos encontrar, então, o vetor $\\overrightarrow{w}$ que minimiza a função $J$. Assim, temos:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_i} = \\frac{\\partial}{\\partial w_i}\\frac{1}{2}\\sum_i^N (y_i - \\widehat{y}_i)^2$$\n",
    "\n",
    "Como a derivada do somatório é igual ao somatório das derivadas:\n",
    "\n",
    "$$= \\frac{1}{2}\\sum_i^N \\frac{\\partial}{\\partial w_i}(y_i - \\widehat{y}_i)^2$$\n",
    "\n",
    "Aplicando a regra da cadeia:\n",
    "\n",
    "$$= \\sum_i^N (y_i - \\widehat{y}_i)\\frac{\\partial}{\\partial w_i}(y_i - \\widehat{y}_i)$$\n",
    "\n",
    "Repare que, quando derivamos $(y_i - \\widehat{y}_i)^2$, o expoente 2, ao sair do somatório, foi multiplicado por $\\frac{1}{2}$, tornando-o 1. Isso é o que os matemáticos denominam de \"conveniência matemática\". \n",
    "\n",
    "Como $\\widehat{y}_i = x_iw_i + b$ é uma função que depende de $w$, e sua derivada em relação a $w_i$ é apenas $x_i$, temos que:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_i} = \\sum_i^N (y_i - \\widehat{y}_i)(-x_i)$$\n",
    "$$\\frac{\\partial J}{\\partial w_i} = -\\sum_i^N (y_i - \\widehat{y}_i)x_i$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\overrightarrow{w}} = -(\\overrightarrow{y} - \\overrightarrow{\\widehat{y}_i})\\overrightarrow{x}$$\n",
    "\n",
    "De maneira análoga, podemos calcular que a derivada de $J$ em relação a $b_i$ é:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b_i} = -\\sum_i^N (y_i - \\widehat{y}_i)*1.0$$\n",
    "\n",
    "Já que a derivada de $\\widehat{y}_i$ em relação a $b_i$ ($\\frac{\\partial J}{\\partial b_i}$) é igual a 1.0. Logo, a atualização dos bias será dada por:\n",
    "\n",
    "$$b_i = b_i + \\lambda(y_i - \\widehat{y}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prova1</th>\n",
       "      <th>prova2</th>\n",
       "      <th>prova3</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>80</td>\n",
       "      <td>75</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93</td>\n",
       "      <td>88</td>\n",
       "      <td>93</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89</td>\n",
       "      <td>91</td>\n",
       "      <td>90</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>96</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73</td>\n",
       "      <td>66</td>\n",
       "      <td>70</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>53</td>\n",
       "      <td>46</td>\n",
       "      <td>55</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>69</td>\n",
       "      <td>74</td>\n",
       "      <td>77</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>47</td>\n",
       "      <td>56</td>\n",
       "      <td>60</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>87</td>\n",
       "      <td>79</td>\n",
       "      <td>90</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>79</td>\n",
       "      <td>70</td>\n",
       "      <td>88</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prova1  prova2  prova3  final\n",
       "0      73      80      75    152\n",
       "1      93      88      93    185\n",
       "2      89      91      90    180\n",
       "3      96      98     100    196\n",
       "4      73      66      70    142\n",
       "5      53      46      55    101\n",
       "6      69      74      77    149\n",
       "7      47      56      60    115\n",
       "8      87      79      90    175\n",
       "9      79      70      88    164"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/notas.csv')\n",
    "\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 3) (25, 1)\n"
     ]
    }
   ],
   "source": [
    "x = df[['prova1', 'prova2', 'prova3']].values\n",
    "y = df['final'].values.reshape(-1, 1)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalizando dados\n",
    "minmax = MinMaxScaler(feature_range=(-1,1))\n",
    "x = minmax.fit_transform(x.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: [501804.28114573]\n",
      "step 200: [173.29461613]\n",
      "step 400: [148.27415652]\n",
      "step 600: [146.38842844]\n",
      "step 800: [146.17957362]\n",
      "step 1000: [146.15407698]\n",
      "step 1200: [146.15100857]\n",
      "step 1400: [146.15067936]\n",
      "step 1600: [146.15065972]\n",
      "step 1800: [146.15066523]\n",
      "step 2000: [146.15066881]\n",
      "w:  [array([8.72518596]), array([14.1394946]), array([26.32051482])]\n",
      "b:  [150.71119402]\n"
     ]
    }
   ],
   "source": [
    "D = x.shape[1]\n",
    "w = [2*random() - 1 for i in range(D)]\n",
    "b = 2*random() - 1\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "for step in range(2001):\n",
    "    cost = 0\n",
    "    for x_n, y_n in zip(x, y):\n",
    "        y_pred = sum([x_i*w_i for x_i, w_i in zip(x_n, w)]) + b\n",
    "        error = y_n - y_pred\n",
    "        w = [w_i + learning_rate*error*x_i for x_i, w_i in zip(x_n, w)]\n",
    "        b = b + learning_rate*error\n",
    "        cost += error**2\n",
    "        \n",
    "    if step%200 == 0:\n",
    "        print('step {0}: {1}'.format(step, cost))\n",
    "\n",
    "print('w: ', w)\n",
    "print('b: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porta AND/OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:11:37.370366Z",
     "start_time": "2017-09-15T11:11:37.359356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2) (4,)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "# y = np.array([[0, 1, 1, 1]]).T # porta OR\n",
    "y = np.array([0, 0, 0, 1]).T # porta AND\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T11:21:18.798586Z",
     "start_time": "2017-09-15T11:21:18.667487Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 4.493956838749234\n",
      "step 10: 0.5668389291255411\n",
      "step 20: 0.3636118127798067\n",
      "step 30: 0.3205007951124835\n",
      "step 40: 0.3109358894855744\n",
      "step 50: 0.30889266952847527\n",
      "step 60: 0.3085388162579939\n",
      "step 70: 0.30853074266129404\n",
      "step 80: 0.3085704206351383\n",
      "step 90: 0.3086018107931739\n",
      "step 100: 0.30862063277586804\n",
      "w:  [0.5550438779427016, 0.5272238323410136]\n",
      "b:  -0.2770587665207685\n"
     ]
    }
   ],
   "source": [
    "D = x.shape[1]\n",
    "w = [2*random() - 1 for i in range(D)]\n",
    "b = 2*random() - 1\n",
    "\n",
    "learning_rate = 0.1 # <- tente estimar a learning_rate\n",
    "\n",
    "for step in range(101):\n",
    "    cost = 0\n",
    "    for x_n, y_n in zip(x, y):\n",
    "        # qual linha devemos remover para transformar o Perceptron num Adaline?\n",
    "        y_pred = sum([x_i*w_i for x_i, w_i in zip(x_n, w)]) + b\n",
    "        #y_pred = 1 if y_pred > 0 else 0\n",
    "        error = y_n - y_pred\n",
    "        w = [w_i + learning_rate*error*x_i for x_i, w_i in zip(x_n, w)]\n",
    "        b = b + learning_rate*error\n",
    "        cost += error**2\n",
    "        \n",
    "    if step%10 == 0:\n",
    "        print('step {0}: {1}'.format(step, cost))\n",
    "\n",
    "print('w: ', w)\n",
    "print('b: ', b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-15T12:21:02.603975Z",
     "start_time": "2017-09-15T12:21:02.555936Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 2.7618590760535247\n",
      "step 10: 0.37057453318143846\n",
      "step 20: 0.31320614430623267\n",
      "step 30: 0.3081820574534263\n",
      "step 40: 0.3081736601436437\n",
      "step 50: 0.3084061759659649\n",
      "step 60: 0.30853310482028684\n",
      "step 70: 0.3085918405260915\n",
      "step 80: 0.30861852290451686\n",
      "step 90: 0.3086308060093753\n",
      "step 100: 0.30863657222211865\n",
      "w:  [0.55543572 0.52766591]\n",
      "b:  -0.2776211885253326\n",
      "y_pred: [-0.27762119  0.25004472  0.27781453  0.80548044]\n"
     ]
    }
   ],
   "source": [
    "D = x.shape[1]\n",
    "w = 2*np.random.random(size=D)-1\n",
    "b = 2*np.random.random()-1       \n",
    "\n",
    "learning_rate = 0.1 # <- use a mesma learning rate do python\n",
    "\n",
    "for step in range(101):\n",
    "    cost = 0\n",
    "    for x_n, y_n in zip(x, y):\n",
    "        # qual linha devemos remover para transformar o Perceptron num Adaline?\n",
    "        y_pred = np.dot(x_n, w) + b \n",
    "        error = y_n - y_pred\n",
    "        w = w + learning_rate*np.dot(error, x_n)\n",
    "        b = b + learning_rate*error\n",
    "        cost += error**2\n",
    "    \n",
    "    if step%10 == 0:\n",
    "        print('step {0}: {1}'.format(step, cost))\n",
    "    \n",
    "print('w: ', w)\n",
    "print('b: ', b)\n",
    "print('y_pred: {0}'.format(np.dot(x, w)+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercício de Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7efd7e96b520>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = make_blobs(n_samples=100, n_features=2, centers=2, random_state=1234)\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "plt.scatter(x[:,0], x[:,1], c=y.ravel(), cmap='bwr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_linear_classifier(x, y, w, b):\n",
    "    x1_min, x1_max = x[:,0].min(), x[:,0].max()\n",
    "    x2_min, x2_max = x[:,1].min(), x[:,1].max()\n",
    "\n",
    "    x1, x2 = np.meshgrid(np.linspace(x1_min-1, x1_max+1,100), np.linspace(x2_min-1, x2_max+1, 100))\n",
    "    x_mesh = np.array([x1.ravel(), x2.ravel()]).T\n",
    "\n",
    "    plt.scatter(x[:,0], x[:,1], c=y.ravel(), cmap='bwr')\n",
    "\n",
    "    y_mesh = np.dot(x_mesh, np.array(w).reshape(1, -1).T) + b\n",
    "    y_mesh = np.where(y_mesh < 0.5, 0, 1)\n",
    "\n",
    "    plt.contourf(x1, x2, y_mesh.reshape(x1.shape), cmap='bwr', alpha=0.5)\n",
    "    plt.xlim(x1_min-1, x1_max+1)\n",
    "    plt.ylim(x2_min-1, x2_max+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = [2*random() - 1 for i in range(D)]\n",
    "b = 2*random() - 1\n",
    "\n",
    "learning_rate = 1.0 # <- tente estimar a learning_rate\n",
    "\n",
    "for step in range(1): # <- tente estimar a #epochs\n",
    "    cost = 0\n",
    "    for x_n, y_n in zip(x, y):\n",
    "        y_pred = sum([x_i*w_i for x_i, w_i in zip(x_n, w)]) + b\n",
    "        error = y_n - y_pred\n",
    "        w = [w_i + learning_rate*error*x_i for x_i, w_i in zip(x_n, w)]\n",
    "        b = b + learning_rate*error\n",
    "        cost += error**2\n",
    "        \n",
    "    if step%100 == 0:\n",
    "        print('step {0}: {1}'.format(step, cost))\n",
    "\n",
    "print('w: ', w)\n",
    "print('b: ', b)\n",
    "\n",
    "plot_linear_classifier(x, y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = x.shape[1]\n",
    "w = 2*np.random.random(size=D)-1\n",
    "b = 2*np.random.random()-1       \n",
    "\n",
    "learning_rate = 1.0 # <- use a mesma learning rate do python\n",
    "\n",
    "for step in range(1): # <- use a mesma #epochs do python\n",
    "    cost = 0\n",
    "    for x_n, y_n in zip(x, y):\n",
    "        y_pred = np.dot(x_n, w) + b \n",
    "        error = y_n - y_pred\n",
    "        w = w + learning_rate*np.dot(error, x_n)\n",
    "        b = b + learning_rate*error\n",
    "        cost += error**2\n",
    "    \n",
    "    if step%100 == 0:\n",
    "        print('step {0}: {1}'.format(step, cost))\n",
    "    \n",
    "print('w: ', w)\n",
    "print('b: ', b)\n",
    "\n",
    "plot_linear_classifier(x, y, w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [http://sisne.org/Disciplinas/PosGrad/PsicoConex/aula6.pdf](http://sisne.org/Disciplinas/PosGrad/PsicoConex/aula6.pdf)\n",
    "- [What is the difference between a Perceptron, Adaline, and neural network model?](https://www.quora.com/What-is-the-difference-between-a-Perceptron-Adaline-and-neural-network-model)\n",
    "- [RNA – Adaline e Regra do Delta](http://www.barbon.com.br/wp-content/uploads/2013/08/RNA_Aula4.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
